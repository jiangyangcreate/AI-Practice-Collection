"""
æ–‡æ¡£é—®ç­”ç³»ç»Ÿ - åŸºäº Gradio å’Œ LangChain çš„ RAG åº”ç”¨
æ”¯æŒ PDF æ–‡æ¡£ä¸Šä¼ å’ŒåŸºäºå†…å®¹çš„æ™ºèƒ½é—®ç­”
"""

import gradio as gr
from pathlib import Path
from typing import List, Tuple, Optional
import tempfile
import shutil

from langchain_community.document_loaders import PyPDFLoader
from langchain_ollama import OllamaEmbeddings
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from ollama import chat, ChatResponse


class DocumentQASystem:
    """æ–‡æ¡£é—®ç­”ç³»ç»Ÿæ ¸å¿ƒç±»"""
    
    def __init__(self, model_name: str = "qwen2.5", chunk_size: int = 1000, chunk_overlap: int = 200):
        """
        åˆå§‹åŒ–æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
        
        Args:
            model_name: Ollama æ¨¡å‹åç§°
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å°
            chunk_overlap: æ–‡æœ¬åˆ†å—é‡å å¤§å°
        """
        self.model_name = model_name
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # åˆå§‹åŒ– embeddings
        self.embeddings = OllamaEmbeddings(model=model_name)
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        self.vector_store = InMemoryVectorStore(embedding=self.embeddings)
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
        )
        
        # å­˜å‚¨å·²åŠ è½½çš„æ–‡æ¡£ä¿¡æ¯
        self.loaded_documents = []
        
    def load_pdf(self, file_path: str) -> List[Document]:
        """
        åŠ è½½å¹¶å¤„ç† PDF æ–‡ä»¶
        
        Args:
            file_path: PDF æ–‡ä»¶è·¯å¾„
            
        Returns:
            åˆ†å‰²åçš„æ–‡æ¡£åˆ—è¡¨
        """
        try:
            loader = PyPDFLoader(file_path)
            documents = loader.load()
            
            # åˆ†å‰²æ–‡æ¡£
            split_docs = self.text_splitter.split_documents(documents)
            
            return split_docs
        except Exception as e:
            raise Exception(f"åŠ è½½ PDF æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def add_documents(self, file_path: str) -> str:
        """
        æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤„ç†ç»“æœä¿¡æ¯
        """
        try:
            # åŠ è½½å¹¶åˆ†å‰²æ–‡æ¡£
            split_docs = self.load_pdf(file_path)
            
            # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            self.vector_store.add_documents(documents=split_docs)
            
            # è®°å½•å·²åŠ è½½çš„æ–‡æ¡£
            file_name = Path(file_path).name
            self.loaded_documents.append({
                'name': file_name,
                'path': file_path,
                'chunks': len(split_docs)
            })
            
            return f"âœ… æˆåŠŸåŠ è½½æ–‡æ¡£: {file_name}\nğŸ“„ å…±åˆ†å‰²ä¸º {len(split_docs)} ä¸ªæ–‡æœ¬å—"
        
        except Exception as e:
            return f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥: {str(e)}"
    
    def retrieve_relevant_docs(self, query: str, k: int = 3) -> List[Document]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        try:
            similar_docs = self.vector_store.similarity_search(query, k=k)
            return similar_docs
        except Exception as e:
            print(f"æ£€ç´¢å¤±è´¥: {str(e)}")
            return []
    
    def generate_answer(self, query: str, context_docs: List[Document]) -> str:
        """
        åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_docs: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£
            
        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([doc.page_content for doc in context_docs])
        
        # æ„å»ºæç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹:
{context}

ç”¨æˆ·é—®é¢˜: {query}

è¯·åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹ç»™å‡ºå‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"""

        try:
            # è°ƒç”¨ Ollama ç”Ÿæˆç­”æ¡ˆ
            response: ChatResponse = chat(
                model=self.model_name,
                messages=[{
                    'role': 'user',
                    'content': prompt,
                }]
            )
            
            return response.message.content
        
        except Exception as e:
            return f"âŒ ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {str(e)}"
    
    def answer_question(self, query: str, k: int = 3) -> Tuple[str, List[str]]:
        """
        å®Œæ•´çš„é—®ç­”æµç¨‹
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            k: æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            (ç­”æ¡ˆ, å‚è€ƒæ–‡æ¡£åˆ—è¡¨)
        """
        if not query.strip():
            return "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚", []
        
        if len(self.loaded_documents) == 0:
            return "âš ï¸ è¯·å…ˆä¸Šä¼ æ–‡æ¡£åå†æé—®ã€‚", []
        
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = self.retrieve_relevant_docs(query, k=k)
        
        if not relevant_docs:
            return "âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹ï¼Œè¯·å°è¯•å…¶ä»–é—®é¢˜ã€‚", []
        
        # ç”Ÿæˆç­”æ¡ˆ
        answer = self.generate_answer(query, relevant_docs)
        
        # æå–å‚è€ƒæ¥æº
        sources = [f"ğŸ“„ æ–‡æ¡£ç‰‡æ®µ {i+1}:\n{doc.page_content[:200]}..." 
                   for i, doc in enumerate(relevant_docs)]
        
        return answer, sources
    
    def get_loaded_documents_info(self) -> str:
        """è·å–å·²åŠ è½½æ–‡æ¡£çš„ä¿¡æ¯"""
        if not self.loaded_documents:
            return "ğŸ“š æš‚æ— å·²åŠ è½½çš„æ–‡æ¡£"
        
        info = "ğŸ“š å·²åŠ è½½çš„æ–‡æ¡£:\n\n"
        for i, doc in enumerate(self.loaded_documents, 1):
            info += f"{i}. {doc['name']} ({doc['chunks']} ä¸ªæ–‡æœ¬å—)\n"
        
        return info
    
    def clear_documents(self):
        """æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£"""
        self.vector_store = InMemoryVectorStore(embedding=self.embeddings)
        self.loaded_documents = []


# å…¨å±€å®ä¾‹
qa_system = DocumentQASystem(model_name="qwen2.5")


def upload_document(file) -> str:
    """å¤„ç†æ–‡æ¡£ä¸Šä¼ """
    if file is None:
        return "âš ï¸ è¯·é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶"
    
    try:
        # è·å–ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
        file_path = file.name if hasattr(file, 'name') else str(file)
        
        # æ·»åŠ æ–‡æ¡£åˆ°ç³»ç»Ÿ
        result = qa_system.add_documents(file_path)
        
        # æ›´æ–°æ–‡æ¡£åˆ—è¡¨æ˜¾ç¤º
        docs_info = qa_system.get_loaded_documents_info()
        
        return f"{result}\n\n{docs_info}"
    
    except Exception as e:
        return f"âŒ ä¸Šä¼ å¤±è´¥: {str(e)}"


def chat_with_documents(message: str, history: List[Tuple[str, str]]) -> str:
    """å¤„ç†èŠå¤©æ¶ˆæ¯"""
    if not message.strip():
        return "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚"
    
    # è·å–ç­”æ¡ˆå’Œå‚è€ƒæ¥æº
    answer, sources = qa_system.answer_question(message, k=3)
    
    # æ„å»ºå®Œæ•´å“åº”
    response = answer
    
    if sources:
        response += "\n\n---\n**ğŸ“š å‚è€ƒæ¥æº:**\n\n"
        response += "\n\n".join(sources)
    
    return response


def clear_all_documents() -> str:
    """æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£"""
    qa_system.clear_documents()
    return "âœ… å·²æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£"


def create_gradio_interface():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    # è‡ªå®šä¹‰ CSS
    custom_css = """
    .gradio-container {
        font-family: 'Arial', sans-serif;
    }
    .upload-box {
        border: 2px dashed #4CAF50 !important;
        border-radius: 10px;
    }
    """
    
    with gr.Blocks(title="ğŸ“š æ–‡æ¡£é—®ç­”ç³»ç»Ÿ", css=custom_css, theme=gr.themes.Soft()) as demo:
        
        gr.Markdown("""
        # ğŸ“š æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
        
        ### åŸºäº RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) æŠ€æœ¯çš„æœ¬åœ°æ–‡æ¡£é—®ç­”åŠ©æ‰‹
        
        **åŠŸèƒ½ç‰¹ç‚¹:**
        - ğŸ“„ æ”¯æŒ PDF æ–‡æ¡£ä¸Šä¼ å’Œè§£æ
        - ğŸ” æ™ºèƒ½æ–‡æ¡£æ£€ç´¢å’Œè¯­ä¹‰åŒ¹é…
        - ğŸ’¬ åŸºäºæ–‡æ¡£å†…å®¹çš„ç²¾å‡†é—®ç­”
        - ğŸ¤– ä½¿ç”¨ Ollama æœ¬åœ°å¤§æ¨¡å‹ (qwen2.5)
        
        ---
        """)
        
        with gr.Row():
            # å·¦ä¾§ï¼šèŠå¤©ç•Œé¢
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ’¬ æ™ºèƒ½é—®ç­”")
                
                chatbot = gr.Chatbot(
                    label="å¯¹è¯å†å²",
                    height=400,
                    bubble_full_width=False,
                    avatar_images=("ğŸ§‘", "ğŸ¤–")
                )
                
                with gr.Row():
                    msg = gr.Textbox(
                        label="è¾“å…¥é—®é¢˜",
                        placeholder="åœ¨è¿™é‡Œè¾“å…¥æ‚¨å…³äºæ–‡æ¡£çš„é—®é¢˜...",
                        scale=4
                    )
                    submit_btn = gr.Button("ğŸš€ å‘é€", variant="primary", scale=1)
                
                gr.Examples(
                    examples=[
                        "è¿™ä»½æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                        "è¯·æ€»ç»“æ–‡æ¡£ä¸­çš„å…³é”®è¦ç‚¹",
                        "æ–‡æ¡£ä¸­æåˆ°äº†å“ªäº›é‡è¦æ¦‚å¿µï¼Ÿ",
                    ],
                    inputs=msg,
                    label="ğŸ’¡ ç¤ºä¾‹é—®é¢˜"
                )
                
                clear_chat_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", size="sm")
            
            # å³ä¾§ï¼šæ–‡æ¡£ç®¡ç†
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“¤ æ–‡æ¡£ç®¡ç†")
                
                file_upload = gr.File(
                    label="ä¸Šä¼  PDF æ–‡æ¡£",
                    file_types=[".pdf"],
                    type="filepath"
                )
                
                upload_status = gr.Textbox(
                    label="ä¸Šä¼ çŠ¶æ€",
                    lines=8,
                    interactive=False,
                    value="ğŸ“š æš‚æ— å·²åŠ è½½çš„æ–‡æ¡£"
                )
                
                with gr.Row():
                    upload_btn = gr.Button("ğŸ“¥ åŠ è½½æ–‡æ¡£", variant="primary")
                    clear_docs_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºæ–‡æ¡£", variant="stop")
                
                gr.Markdown("""
                ---
                **ä½¿ç”¨è¯´æ˜:**
                1. ä¸Šä¼  PDF æ–‡æ¡£
                2. ç‚¹å‡»"åŠ è½½æ–‡æ¡£"æŒ‰é’®
                3. åœ¨å·¦ä¾§è¾“å…¥é—®é¢˜å¼€å§‹å¯¹è¯
                
                **æ³¨æ„:** é¦–æ¬¡ä½¿ç”¨éœ€è¦ç¡®ä¿å·²å®‰è£… Ollama å¹¶ä¸‹è½½ qwen2.5 æ¨¡å‹
                """)
        
        # äº‹ä»¶å¤„ç†
        def user_message(message, history):
            """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
            return "", history + [[message, None]]
        
        def bot_response(history):
            """ç”Ÿæˆæœºå™¨äººå“åº”"""
            user_msg = history[-1][0]
            bot_msg = chat_with_documents(user_msg, history[:-1])
            history[-1][1] = bot_msg
            return history
        
        # ç»‘å®šäº‹ä»¶
        msg.submit(user_message, [msg, chatbot], [msg, chatbot]).then(
            bot_response, chatbot, chatbot
        )
        submit_btn.click(user_message, [msg, chatbot], [msg, chatbot]).then(
            bot_response, chatbot, chatbot
        )
        
        upload_btn.click(upload_document, file_upload, upload_status)
        clear_docs_btn.click(clear_all_documents, None, upload_status)
        clear_chat_btn.click(lambda: None, None, chatbot)
        
        # å¯åŠ¨æ—¶æ˜¾ç¤ºæ–‡æ¡£ä¿¡æ¯
        demo.load(lambda: qa_system.get_loaded_documents_info(), None, upload_status)
    
    return demo


if __name__ == "__main__":
    print("ğŸš€ æ­£åœ¨å¯åŠ¨æ–‡æ¡£é—®ç­”ç³»ç»Ÿ...")
    print("ğŸ“Œ è¯·ç¡®ä¿å·²å®‰è£… Ollama å¹¶ä¸‹è½½äº† qwen2.5 æ¨¡å‹")
    print("   å®‰è£…æ–¹æ³•: ollama pull qwen2.5")
    print()
    import webbrowser
    webbrowser.open("http://localhost:7860")
    demo = create_gradio_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )

